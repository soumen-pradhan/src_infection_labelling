{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea827e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dia_matrix\n",
    "import networkx as nx\n",
    "from networkx import shortest_path_length as nx_path\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import itertools\n",
    "import statistics as stats\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from sys import maxsize\n",
    "\n",
    "np.set_printoptions(threshold=maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae19c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" all simulation and label functions return ([label-vector], runtime) \"\"\"\n",
    "\n",
    "\n",
    "def simulateInfection(G, src, model=\"SI\", lamda=0.3, threshold=0.3):\n",
    "    N = G.number_of_nodes()\n",
    "    nodes = list(G.nodes())\n",
    "    infected_nodes = {src}\n",
    "\n",
    "    while True:\n",
    "        temp_infected = infected_nodes.copy()\n",
    "\n",
    "        for node in infected_nodes:\n",
    "            for neighbour in G.neighbors(node):\n",
    "                if rand.random() < lamda:\n",
    "                    temp_infected.add(neighbour)\n",
    "\n",
    "        infected_nodes = temp_infected\n",
    "\n",
    "        if len(infected_nodes) > threshold * N:\n",
    "            break\n",
    "\n",
    "    y = np.full(N, -1)\n",
    "    for infNode in infected_nodes:\n",
    "        y[nodes.index(infNode)] = 1\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05640772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def labelRankingScore(G, y, alpha=0.5):\n",
    "    start = timer()\n",
    "    N = G.number_of_nodes()\n",
    "\n",
    "    W = nx.adjacency_matrix(G)\n",
    "    diag_elem = W.sum(axis=1).A1 ** (-0.5)\n",
    "\n",
    "    inv_sqrt_D = dia_matrix((diag_elem, [0]), shape=W.get_shape())\n",
    "    S = inv_sqrt_D @ W @ inv_sqrt_D\n",
    "\n",
    "    f = np.copy(y)\n",
    "\n",
    "    while True:\n",
    "        f_ = alpha * S @ f + (1 - alpha) * y\n",
    "\n",
    "        if np.linalg.norm(f - f_) < 0.001 * N:  # Convergence Criteria\n",
    "            break\n",
    "\n",
    "        f = f_\n",
    "\n",
    "    tup = [(n, f[i]) for i, n in enumerate(G.nodes())]\n",
    "\n",
    "    end = timer()\n",
    "    return dict(tup), end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1191fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" known_dicts = [ {infected: [nodes], safe: [nodes]} ] \"\"\"\n",
    "\n",
    "\n",
    "def simulatePartialInfection(\n",
    "    G, src, model=\"SI\", lamda=0.3, threshold=0.3, sampling=0.7, n_snaps=1\n",
    "):\n",
    "    N = G.number_of_nodes()\n",
    "    nodes = list(G.nodes())\n",
    "    infected = {src}\n",
    "\n",
    "    while True:\n",
    "        temp_infected = infected.copy()\n",
    "\n",
    "        for node in infected:\n",
    "            for neighbour in nx.all_neighbors(G, node):\n",
    "                if rand.random() < lamda:\n",
    "                    temp_infected.add(neighbour)\n",
    "\n",
    "        infected = temp_infected\n",
    "\n",
    "        if len(infected) > threshold * N:\n",
    "            break\n",
    "\n",
    "    snapshots = [rand.sample(nodes, int(sampling * N)) for _ in range(n_snaps)]\n",
    "    known_dicts = [{\"infected\": [], \"safe\": []} for _ in range(n_snaps)]\n",
    "\n",
    "    for i, snapshot in enumerate(snapshots):\n",
    "        for node in snapshot:\n",
    "            known_dicts[i][\"infected\" if node in infected else \"safe\"].append(node)\n",
    "\n",
    "    return known_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022cca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resetF(F, known_dict, nodes):\n",
    "    for node in known_dict[\"safe\"]:\n",
    "        indx = nodes.index(node)\n",
    "        F[indx][0] = 1\n",
    "        F[indx][1] = 0\n",
    "\n",
    "    for node in known_dict[\"infected\"]:\n",
    "        indx = nodes.index(node)\n",
    "        F[indx][0] = 0\n",
    "        F[indx][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14c77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GFHF(G, known_dict):\n",
    "    start = timer()\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    N = G.number_of_nodes()\n",
    "    W = nx.adjacency_matrix(G)\n",
    "\n",
    "    diag_elem = 1 / W.sum(axis=1).A1\n",
    "    inv_D = dia_matrix((diag_elem, [0]), shape=W.get_shape())\n",
    "\n",
    "    P = inv_D @ W\n",
    "    F = np.zeros((N, 2))\n",
    "\n",
    "    resetF(F, known_dict, nodes)\n",
    "\n",
    "    while True:\n",
    "        F_ = P @ F\n",
    "        curr_diff = sum(sum(abs(F - F_)))\n",
    "\n",
    "        resetF(F, known_dict, nodes)\n",
    "\n",
    "        if curr_diff < 0.0001 * N:  # Convergence Criteria\n",
    "            break\n",
    "\n",
    "        F = F_\n",
    "\n",
    "    O = np.array([1 if f[1] > f[0] else -1 for f in F])\n",
    "\n",
    "    end = timer()\n",
    "    return O, end - start\n",
    "\n",
    "\n",
    "def TSSI_GFHF(G, labelled, src):\n",
    "    O, partial_time = GFHF(G, labelled)\n",
    "    dict_scores, complete_time = labelRankingScore(G, O)\n",
    "\n",
    "    pred_src = max(dict_scores, key=dict_scores.get)\n",
    "    dist_err = nx_path(G, src, pred_src)\n",
    "    return dist_err, partial_time + complete_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1ff341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LGC(G, known_dict, alpha):\n",
    "    start = timer()\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    N = G.number_of_nodes()\n",
    "    W = nx.adjacency_matrix(G)\n",
    "\n",
    "    diag_elem = W.sum(axis=1).A1 ** (-0.5)\n",
    "    inv_sqrt_D = dia_matrix((diag_elem, [0]), shape=W.get_shape())\n",
    "\n",
    "    S = inv_sqrt_D @ W @ inv_sqrt_D\n",
    "    F = np.zeros((N, 2))\n",
    "\n",
    "    resetF(F, known_dict, nodes)\n",
    "    Y = np.copy(F)\n",
    "\n",
    "    while True:\n",
    "        F_ = alpha * S @ F + (1 - alpha) * Y\n",
    "\n",
    "        if sum(sum(abs(F - F_))) < 0.0001 * N:  # Convergence Criterion\n",
    "            break\n",
    "\n",
    "        F = F_\n",
    "\n",
    "    O = np.array([1 if f[1] > f[0] else -1 for f in F])\n",
    "\n",
    "    end = timer()\n",
    "    return O, end - start\n",
    "\n",
    "\n",
    "def TSSI_LGC(G, labelled, src, alpha=0.5):\n",
    "    O, partial_time = LGC(G, labelled, alpha=alpha)\n",
    "    dict_scores, complete_time = labelRankingScore(G, O)\n",
    "\n",
    "    pred_src = max(dict_scores, key=dict_scores.get)\n",
    "    dist_err = nx_path(G, src, pred_src)\n",
    "\n",
    "    return dist_err, partial_time + complete_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17262d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with open(\"football.csv\", \"r\") as data:\n",
    "    next(data, None)\n",
    "    G_football = nx.parse_edgelist(\n",
    "        data, delimiter=\",\", create_using=nx.Graph, nodetype=int\n",
    "    )\n",
    "\n",
    "with open(\"facebook.csv\", \"r\") as data:\n",
    "    next(data, None)\n",
    "    G_facebook = nx.parse_edgelist(\n",
    "        data, delimiter=\",\", create_using=nx.Graph, nodetype=int\n",
    "    )\n",
    "\n",
    "d_url = \"http://www-personal.umich.edu/~mejn/netdata/dolphins.zip\"\n",
    "with urlopen(d_url) as sock, ZipFile(BytesIO(sock.read())) as zf:\n",
    "    gml = zf.read(\"dolphins.gml\").decode().split(\"\\n\")[1:]\n",
    "    G_dolphin = nx.parse_gml(gml)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b013d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "# Counter -> dict {item: freq}\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Creating dataframes consisting of error_distance, time, and error_distance_frequncy for \\\n",
    "# for different datasets for different algorithms\n",
    "def gen_data(algo, dataset, n_snaps=10):\n",
    "\n",
    "    df_dist_err = defaultdict(list)\n",
    "    df_time = defaultdict(list)\n",
    "    err_freq = []\n",
    "\n",
    "    for G_name, G in dataset.items():\n",
    "        src = rand.choice(list(G.nodes()))\n",
    "        N = G.number_of_nodes()\n",
    "\n",
    "        known_dicts = simulatePartialInfection(\n",
    "            G, src, threshold=0.3, sampling=0.2, n_snaps=n_snaps\n",
    "        )\n",
    "            \n",
    "        score_time = [\n",
    "            [alg(G, label, src) for label in known_dicts] for alg in algo.values()\n",
    "        ]\n",
    "\n",
    "        \"\"\"\n",
    "        alg_data = [alg1 [], alg2 [] ...]\n",
    "        alg1 [] -> [(dist_err, snapshot_time)]\n",
    "        snapshot_score_dict = {node: score}\n",
    "        \"\"\"\n",
    "\n",
    "        alg_data = [list(zip(*res)) for res in score_time]\n",
    "#         print(alg_data)\n",
    "        \n",
    "        dict_freq = defaultdict(list)\n",
    "        \n",
    "        freq_hops = [Counter(err) for err, _ in alg_data]\n",
    "        avg_dist_err = [stats.mean(err) for err, _ in alg_data]\n",
    "        avg_time = [stats.mean(time) for _, time in alg_data]\n",
    "\n",
    "        for alg_name, de, time, freq in zip(algo.keys(), avg_dist_err, avg_time, freq_hops):\n",
    "            df_dist_err[alg_name].append(de)\n",
    "            df_time[alg_name].append(time)\n",
    "            dict_freq[alg_name] = [freq[i] if i in freq else 0 for i in range(4)]\n",
    "        \n",
    "        df = pd.DataFrame(dict_freq, columns=algo.keys(), index=list(range(4)))\n",
    "        err_freq.append(df)\n",
    "    \n",
    "    df_de = pd.DataFrame(df_dist_err, columns=algo.keys(), index=dataset.keys())\n",
    "    df_time = pd.DataFrame(df_time, columns=algo.keys(), index=dataset.keys())\n",
    "                        \n",
    "    return df_de, df_time, err_freq\n",
    "\n",
    "\n",
    "de, time, err_freq = gen_data(\n",
    "    {\"GFHF\": TSSI_GFHF, \"LGC\": TSSI_LGC},\n",
    "    {\n",
    "        \"Karate\": nx.karate_club_graph(),\n",
    "        \"Football\": G_football,\n",
    "        'Facebook': G_facebook,\n",
    "        'Dolphin': G_dolphin\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GMLA_2 import *\n",
    "from PTVA_algo_final2 import *\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "algo = {\"GMLA\": GMLA}\n",
    "dataset =  {\n",
    "            \"Karate\": nx.karate_club_graph(),\n",
    "            \"Football\": G_football,\n",
    "            'Facebook': G_facebook,\n",
    "            'Dolphin': G_dolphin\n",
    "        }\n",
    "\n",
    "def gen_data_2(dataset, algo, iterations):\n",
    "    df_dist_err = defaultdict(list)\n",
    "    df_time = defaultdict(list)\n",
    "    err_freq = []\n",
    "\n",
    "    for G_name, G in dataset.items():\n",
    "        N = G.number_of_nodes()\n",
    "        score_time = [alg(G, G_name, iterations) for alg in algo.values()]\n",
    "        alg_data = [list(zip(*res)) for res in score_time]\n",
    "\n",
    "        dict_freq = defaultdict(list)\n",
    "\n",
    "        freq_hops = [Counter(err) for err, _ in alg_data]\n",
    "        avg_dist_err = [stats.mean(err) for err, _ in alg_data]\n",
    "        avg_time = [stats.mean(time) for _, time in alg_data]\n",
    "        \n",
    "        for alg_name, de, time, freq in zip(algo.keys(), avg_dist_err, avg_time, freq_hops):\n",
    "            df_dist_err[alg_name].append(de)\n",
    "            df_time[alg_name].append(time)\n",
    "            dict_freq[alg_name] = [freq[i] if i in freq else 0 for i in range(4)]\n",
    "\n",
    "        df = pd.DataFrame(dict_freq, columns=algo.keys(), index=list(range(4)))\n",
    "        err_freq.append(df)\n",
    "\n",
    "    df_de = pd.DataFrame(df_dist_err, columns=algo.keys(), index=dataset.keys())\n",
    "    df_time = pd.DataFrame(df_time, columns=algo.keys(), index=dataset.keys())\n",
    "\n",
    "    return df_de, df_time, err_freq\n",
    "        \n",
    "de1, time1, freq1 = gen_data_2(dataset, algo, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f0f5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          GFHF  LGC  GMLA\n",
      "Karate     1.8  1.5   1.0\n",
      "Football   1.5  0.6   2.6\n",
      "Facebook   1.6  1.6   NaN\n",
      "Dolphin    2.3  1.3   NaN\n"
     ]
    }
   ],
   "source": [
    "de_final = pd.concat([de, de1], axis=1)\n",
    "print(de_final)\n",
    "time_final = pd.concat([time, time1], axis=1)\n",
    "\n",
    "# de.plot.bar(xlabel=\"Datasets\", ylabel=\"avg_distance_error\", title=\"Average Distance Error\")\n",
    "# plt.show()\n",
    "\n",
    "# time.plot.bar()\n",
    "# plt.show()\n",
    "\n",
    "# for df in err_freq:\n",
    "#     df.plot.bar()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdad995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
